{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a98a7146",
   "metadata": {},
   "source": [
    "# Converting RSS data into Data cubes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845128f2",
   "metadata": {},
   "source": [
    "This notebook contains a basic explanation of the main cube interpolation methods available in PyKOLA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2e1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from pykoala import __version__\n",
    "from astropy import units as u\n",
    "\n",
    "print(\"We are using pyKOALA version: \", __version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f2c139",
   "metadata": {},
   "source": [
    "For this tutorial, no corrections will be applied to the data. The main focus will consist of showcasing the different cubing approaches and some intrinsic differences.\n",
    "\n",
    "First, let us start by reading some RSS data consisting of a a set of individual exposures of the same object following a dither pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c95d24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykoala.instruments.koala_ifu import koala_rss\n",
    "\n",
    "sci_rss = []\n",
    "for i in [34, 35, 36]:\n",
    "    filename = f\"data/27feb200{i}red.fits\"\n",
    "    rss = koala_rss(filename)\n",
    "    sci_rss.append(rss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b700778a",
   "metadata": {},
   "source": [
    "As in previous tutorials, the first step is to build the WCS that will define the datacube structure. For this example, we will simply construct a datacube that uses the same spectral sampling as the RSS data, whereas for the spatial dimensions we will sample the data into a (40, 60) matrix with a pixels size of 0.5 arcseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae822f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykoala.cubing import build_cube, build_wcs, build_cube, build_wcs_from_rss\n",
    "\n",
    "# Number of pixels along the three dimensions\n",
    "datacube_shape = (sci_rss[0].wavelength.size, 40, 60)\n",
    "# Reference position along the three axes.\n",
    "ref_position = (sci_rss[0].wavelength[0], np.mean(sci_rss[0].info['fib_ra']),\n",
    "                np.mean(sci_rss[0].info['fib_dec']))  # (deg, deg)\n",
    "# Spatial pixel scale size in deg\n",
    "spatial_pixel_size = 0.5 << u.arcsec\n",
    "# Spectral pixel scale in angstrom\n",
    "spectral_pixel_size = sci_rss[0].wavelength[1] - sci_rss[0].wavelength[0]\n",
    "\n",
    "# wcs = build_wcs(datacube_shape=datacube_shape,\n",
    "#                 reference_position=ref_position,\n",
    "#                 spatial_pix_size=spatial_pixel_size,\n",
    "#                 spectra_pix_size=spectral_pixel_size,\n",
    "#             )\n",
    "\n",
    "wcs = build_wcs_from_rss(sci_rss, spatial_pix_size=spatial_pixel_size, spectra_pix_size=spectral_pixel_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb812857",
   "metadata": {},
   "source": [
    "Once we have defined the `WCS`, the next step is to define the cubing method to used to combine the RSS data. For this, there are two aspects to keep in mind. The first one is the choice of the kernel to interpolate our data along the spacial directions of the data cube. On the other hand, we need to choose how do we want to combine the information of each individual RSS after they have been sampled into a datacube. The two main objectives here are maximizin the signal to noise ratio, while keeping outliers to a minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cfe95",
   "metadata": {},
   "source": [
    "### Choosing a Kernel\n",
    "\n",
    "The choice of an interpolation kernel and its characteristic scale depends on many factors such as:\n",
    "- The number of fibres in the RSS data\n",
    "- The number of individual exposures (RSS) to combine.\n",
    "- The projected size of the fibres\n",
    "- The pixel size of the output datacube.\n",
    "- The seeing of the observations\n",
    "\n",
    "Unfortunately, there is no general rule when it comes to chosing a particular interpolation scheme. Regardless of the kernel shape, its scale should always range around the fibre size or the seeing of the night. The more individual exposures we have available to combine, the smaller the scale can be in order to recover more structure.\n",
    "\n",
    "In PyKOALA, there are available a series of interpolation kernels that can be used to combine RSS exposures. All kernels are defined by `scale` parameter, which represents the spatial extent to which individual fibres contribute to the datacube grid of spaxels. When the kernel function extends to infinity (e.g. a Gaussian), it is also possible to define a `truncation_radius`, which prevents the data to be interpolated over the whole datacube (minimizing the resultant covariance between the pixels).\n",
    "\n",
    "In the example below, we will define four kernels:\n",
    "- A Gaussian (`GaussianKernel`).\n",
    "- A parabolic or Epanechnikov kernel (`ParabolicKernel`).\n",
    "- A Top Hat kernel (`TopHat`).\n",
    "- A Drizzling kernel (`DrizzlingKernel`).\n",
    "\n",
    "The latter enables to perform the traditional drizzling algorithm (which can be interpreted as an interpolation using a Top Hat kernel). In particular, this algorithm consists of defining a set of input pixels, shrinking them by a given scale, and then including them in the final datacube grid accounting for the overlapping fraction of between the input and target pixels. To date, this algorithm assumes that the input RSS fibres are circular, and computes the overlap fraction of the circular fibre aperture with the square pixels of the datacube.\n",
    "\n",
    "Now, let's have a look at their functional form along in one dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8294a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykoala.cubing import GaussianKernel, ParabolicKernel, TopHatKernel, DrizzlingKernel\n",
    "\n",
    "pixel_scale = 0.7\n",
    "truncation_radius = 2.0\n",
    "gauss_kernel = GaussianKernel(scale=pixel_scale, truncation_radius=truncation_radius)\n",
    "epanechikov_kernel = ParabolicKernel(scale=pixel_scale)  # This kernel's domain is confined within +/-1\n",
    "tophat_kernel = TopHatKernel(scale=pixel_scale, truncation_radius=truncation_radius)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2d156c",
   "metadata": {},
   "source": [
    "The `truncation radius` is expressed in units of the `scale`. In other words, if our characteristic scale is 0.7 pixels, and our truncation radius is 2, then the pixels laying beyond 1.4 pixels with respect to the kernel centre will have a null weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd804c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dummy_pixel_edges = np.arange(-2.5, 2.5, 0.01)\n",
    "\n",
    "dummy_pixel_bins = (dummy_pixel_edges[:-1] + dummy_pixel_edges[1:]) / 2\n",
    "\n",
    "plt.figure(figsize=(10, 5), constrained_layout=True)\n",
    "plt.subplot(121)\n",
    "plt.plot(dummy_pixel_bins, gauss_kernel.kernel_1D(dummy_pixel_edges), label='Gaussian')\n",
    "plt.plot(dummy_pixel_bins, epanechikov_kernel.kernel_1D(dummy_pixel_edges), label='Parabolic')\n",
    "plt.plot(dummy_pixel_bins, tophat_kernel.kernel_1D(dummy_pixel_edges), label='TopHat')\n",
    "plt.axvline(-pixel_scale * truncation_radius, color='k',  ls='--', alpha=0.5)\n",
    "plt.axvline(pixel_scale * truncation_radius, color='k',  ls='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.xlabel('Relative pixel position')\n",
    "plt.ylabel('PDF')\n",
    "plt.subplot(122)\n",
    "plt.plot(dummy_pixel_edges, gauss_kernel.cmf(dummy_pixel_edges / 0.7), label='Gaussian')\n",
    "plt.plot(dummy_pixel_edges, epanechikov_kernel.cmf(dummy_pixel_edges / 0.7), label='Parabolic')\n",
    "plt.plot(dummy_pixel_edges, tophat_kernel.cmf(dummy_pixel_edges / 0.7), label='TopHat')\n",
    "plt.axvline(-pixel_scale * truncation_radius, color='k',  ls='--', alpha=0.5)\n",
    "plt.axvline(pixel_scale * truncation_radius, color='k',  ls='--', alpha=0.5, label='Truncation radius')\n",
    "plt.xlabel('Relative pixel position')\n",
    "plt.ylabel('Cumulative density function')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e817779c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcs.celestial.wcs.cunit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b140474b",
   "metadata": {},
   "source": [
    "#### Chosing the stacking method\n",
    "\n",
    "In addition to the kernel, we must also decide how do we want to combine the individual exposures (in case there is more than one) into the final data cube. For this PyKOALA currently provides two flavours:\n",
    "- sigma clipping\n",
    "- MAD clipping\n",
    "\n",
    "For more information see the documentation of `CubeStacking` and its methods.\n",
    "\n",
    "If you want to use you own stacking method, you can implement a function that takes as its two first arguments a list of datacubes (3D arrays), and a list of variances (idem), followed by keyword arguments and returns the combined cube and variance.\n",
    "\n",
    "Finally, let us combine the RSS data into a datacube using different interpolation kernels. For each kernel, we will use the method `build_cube`, which will return a `pykoala.cubing.Cube` object, and a dictionary containing several QC plots (exposure time maps and fibre coverage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc8d5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cubes = []\n",
    "for kernel in [DrizzlingKernel, ParabolicKernel, GaussianKernel]:\n",
    "    print(\"Building cube using kernel: \", kernel.__name__)\n",
    "    new_cube, plots = build_cube(rss_set=sci_rss,\n",
    "                            wcs=wcs,\n",
    "                            kernel=kernel,\n",
    "                            kernel_size_arcsec=2.0,\n",
    "                            kernel_truncation_radius=1,\n",
    "                            qc_plots=True)\n",
    "    all_cubes.append(new_cube)\n",
    "    \n",
    "    plt.figure(plots['rss_1'])\n",
    "    plt.figure(plots['weights'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f362390",
   "metadata": {},
   "source": [
    "Let us now have a look at the differences between the different cubing methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb0ea52",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl_idx = 400\n",
    "\n",
    "fig, axs = plt.subplots(nrows=2, ncols=4, figsize=(12, 8), constrained_layout=True)\n",
    "im_args = dict(vmax=0.3, vmin=-3, interpolation='none', cmap='nipy_spectral')\n",
    "snr_args = dict(vmax=3, vmin=0, interpolation='none', cmap='nipy_spectral')\n",
    "\n",
    "names = ['Drizzling', 'Parabolic', 'Gaussian']\n",
    "for i in range(3):\n",
    "    ax = axs[0, i]\n",
    "    ax.set_title(names[i])\n",
    "    mappable = ax.imshow(np.log10(all_cubes[i].intensity[wl_idx].value), **im_args)\n",
    "    plt.colorbar(mappable, ax=ax,\n",
    "                 label=f'log(Flux / {all_cubes[i].intensity[wl_idx].unit})')\n",
    "    ax = axs[1, i]\n",
    "    mappable = ax.imshow(np.log10(all_cubes[i].intensity[wl_idx]\n",
    "                       / all_cubes[i].variance[wl_idx]**0.5), **snr_args)\n",
    "    plt.colorbar(mappable, ax=ax, label='log(SNR)')\n",
    "\n",
    "    ax = axs[0, -1]\n",
    "    ax.plot(np.nanmean(all_cubes[i].intensity[wl_idx], axis=0), label=names[i])\n",
    "    ax = axs[1, -1]\n",
    "    ax.plot(\n",
    "        np.nanmean(all_cubes[i].intensity[wl_idx], axis=0\n",
    "                  ) / np.nanmean(all_cubes[i].variance[wl_idx], axis=0)**0.5)\n",
    "\n",
    "axs[0, -1].set_yscale('symlog')\n",
    "axs[0, -1].legend(bbox_to_anchor=(0.5, 1.05), loc='lower center')\n",
    "axs[1, -1].set_yscale('symlog')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
